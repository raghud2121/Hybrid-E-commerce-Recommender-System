{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf2985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/b2/3b/47b5eaee01ef2b5a80ba3f7f6ecf79587cb458690857d4777bfd77371c6f/scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\raghu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (2.3.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\raghu\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.16.1)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Obtaining dependency information for joblib>=1.2.0 from https://files.pythonhosted.org/packages/7d/4f/1195bbac8e0c2acc5f740661631d8d750dc38d4a32b23ee5df3cde6f4e0d/joblib-1.5.1-py3-none-any.whl.metadata\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/32/d5/f9a850d79b0851d1d4ef6456097579a9005b31fea68726a4ae5f2d82ddd9/threadpoolctl-3.6.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.1-cp311-cp311-win_amd64.whl (8.9 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.1 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\raghu\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fb09cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gzip\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b6b73a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data in chunks to conserve memory...\n",
      "Concatenating sampled chunks into a final DataFrame...\n",
      "Data loaded successfully. Final sample size: 17680\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# --- Parameters ---\n",
    "file_path = 'data/Electronics_5.json'\n",
    "chunk_size = 100000  # Process 100,000 lines at a time\n",
    "sample_size_per_chunk = 260 # Sample this many reviews from each chunk\n",
    "\n",
    "list_of_sampled_chunks = []\n",
    "\n",
    "print(\"Loading data in chunks to conserve memory...\")\n",
    "# Create an iterator that reads the file in chunks instead of all at once\n",
    "try:\n",
    "    with pd.read_json(file_path, lines=True, chunksize=chunk_size) as json_reader:\n",
    "        for chunk in json_reader:\n",
    "            # Keep only the columns we need\n",
    "            chunk_filtered = chunk[['reviewerID', 'asin', 'overall', 'reviewText']]\n",
    "\n",
    "            # Take a random sample from this chunk to build our final dataset\n",
    "            list_of_sampled_chunks.append(chunk_filtered.sample(n=sample_size_per_chunk, random_state=42))\n",
    "\n",
    "except ValueError:\n",
    "    # This handles if the last chunk is smaller than the sample size\n",
    "    print(\"Reached end of file.\")\n",
    "\n",
    "print(\"Concatenating sampled chunks into a final DataFrame...\")\n",
    "# Combine all the small, sampled chunks into one\n",
    "df_sample = pd.concat(list_of_sampled_chunks, ignore_index=True)\n",
    "\n",
    "# Rename columns for consistency with the rest of the project\n",
    "df_sample.rename(columns={'reviewerID': 'user_id', 'asin': 'item_id', 'overall': 'rating'}, inplace=True)\n",
    "\n",
    "print(f\"Data loaded successfully. Final sample size: {len(df_sample)}\")\n",
    "df_sample.head()\n",
    "# Add this line at the end of Cell 2\n",
    "df_sample.to_pickle('saved_models/df_sample.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "920adb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF matrix from review text...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raghu\\AppData\\Local\\Temp\\ipykernel_16580\\3119606799.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df_sample['reviewText'].fillna('', inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF models created and saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create a TF-IDF matrix directly from the raw review text\n",
    "# It will automatically handle tokenization and remove common English stop words.\n",
    "print(\"Creating TF-IDF matrix from review text...\")\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000, \n",
    "    stop_words='english',\n",
    "    ngram_range=(1, 2) # Also consider two-word phrases like \"battery life\"\n",
    ")\n",
    "\n",
    "# Handle potential empty reviews by filling them with an empty string\n",
    "df_sample['reviewText'].fillna('', inplace=True)\n",
    "item_features = tfidf.fit_transform(df_sample['reviewText'])\n",
    "\n",
    "# Save the TF-IDF vectorizer and the item features matrix\n",
    "pickle.dump(tfidf, open('saved_models/tfidf_vectorizer.pkl', 'wb'))\n",
    "pickle.dump(item_features, open('saved_models/item_features.pkl', 'wb'))\n",
    "\n",
    "# Also save the mapping from dataframe index to item_id\n",
    "item_id_map = df_sample['item_id']\n",
    "item_id_map.to_pickle('saved_models/item_id_map.pkl')\n",
    "\n",
    "print(\"TF-IDF models created and saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64c92821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a sparse user-item matrix to conserve memory...\n",
      "Training TruncatedSVD model...\n",
      "Scikit-learn based collaborative filtering models saved.\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "print(\"Creating a sparse user-item matrix to conserve memory...\")\n",
    "\n",
    "# Step 1: Create integer mappings for users and items for the sparse matrix\n",
    "# This is essential because sparse matrices require integer indices.\n",
    "user_c = pd.Categorical(df_sample['user_id'])\n",
    "item_c = pd.Categorical(df_sample['item_id'])\n",
    "\n",
    "# The 'codes' attribute gives us the integer index for each user/item\n",
    "user_codes = user_c.codes\n",
    "item_codes = item_c.codes\n",
    "\n",
    "# Step 2: Create the sparse matrix\n",
    "# csr_matrix((data, (row_indices, col_indices)), shape)\n",
    "user_item_sparse_matrix = csr_matrix(\n",
    "    (df_sample['rating'], (user_codes, item_codes)),\n",
    "    shape=(len(user_c.categories), len(item_c.categories))\n",
    ")\n",
    "\n",
    "# The SVD model can now run on this memory-efficient sparse matrix\n",
    "print(\"Training TruncatedSVD model...\")\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "matrix_decomposed = svd.fit_transform(user_item_sparse_matrix)\n",
    "\n",
    "# Get the correlation matrix of the decomposed user-feature matrix\n",
    "corr_matrix = np.corrcoef(matrix_decomposed)\n",
    "\n",
    "# We need to save the maps from integer codes back to original IDs\n",
    "user_id_map = {code: user for code, user in enumerate(user_c.categories)}\n",
    "\n",
    "# Save the necessary objects for the API\n",
    "pickle.dump(corr_matrix, open('saved_models/corr_matrix.pkl', 'wb'))\n",
    "pickle.dump(user_id_map, open('saved_models/user_id_map_cf.pkl', 'wb'))\n",
    "\n",
    "print(\"Scikit-learn based collaborative filtering models saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
